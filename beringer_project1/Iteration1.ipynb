{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration 1 of the CNN on PYNQ with FINN Framework\n",
    "\n",
    "This notebook should give an overview of the ONNX model we are using throughout this project.  Steps include training this model to our liking, implementing the model in HLS using the FINN framework, and generating a functional Vivado Block Diagram and Bitstream compatable with the PYNQ development board."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "* #### Initial ONNX model\n",
    "* #### Manipulation of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Model\n",
    "\n",
    "First ONNX is imported, then the helper function can be used to make a node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "Add1_node = onnx.helper.make_node(\n",
    "    'Add',\n",
    "    inputs=['in1', 'in2'],\n",
    "    outputs=['sum1'],\n",
    "    name='Add1'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first attribute of the node is the operation type. In this case it is `'Add'`, so it is an adder node. Then the input names are passed to the node and at the end a name is assigned to the output.\n",
    "    \n",
    "For this example we want two other adder nodes, one abs node and the output shall be rounded so one round node is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Add2_node = onnx.helper.make_node(\n",
    "    'Add',\n",
    "    inputs=['sum1', 'in3'],\n",
    "    outputs=['sum2'],\n",
    "    name='Add2',\n",
    ")\n",
    "\n",
    "Add3_node = onnx.helper.make_node(\n",
    "    'Add',\n",
    "    inputs=['abs1', 'abs1'],\n",
    "    outputs=['sum3'],\n",
    "    name='Add3',\n",
    ")\n",
    "\n",
    "Abs_node = onnx.helper.make_node(\n",
    "    'Abs',\n",
    "    inputs=['sum2'],\n",
    "    outputs=['abs1'],\n",
    "    name='Abs'\n",
    ")\n",
    "\n",
    "Round_node = onnx.helper.make_node(\n",
    "    'Round',\n",
    "    inputs=['sum3'],\n",
    "    outputs=['out1'],\n",
    "    name='Round',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "in1 = onnx.helper.make_tensor_value_info(\"in1\", onnx.TensorProto.FLOAT, [4, 4])\n",
    "in2 = onnx.helper.make_tensor_value_info(\"in2\", onnx.TensorProto.FLOAT, [4, 4])\n",
    "in3 = onnx.helper.make_tensor_value_info(\"in3\", onnx.TensorProto.FLOAT, [4, 4])\n",
    "out1 = onnx.helper.make_tensor_value_info(\"out1\", onnx.TensorProto.FLOAT, [4, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the graph can be built. First all nodes are passed. Here it is to be noted that it requires a certain sequence. The nodes must be instantiated in their dependencies to each other. This means Add2 must not be listed before Add1, because Add2 depends on the result of Add1. A name is then assigned to the graph. This is followed by the inputs and outputs. \n",
    "\n",
    "`value_info` of the graph contains the remaining tensors within the graph. When creating the nodes we have already defined names for the inner data edges and now these are assigned tensors of the datatype float and a certain shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " graph = onnx.helper.make_graph(\n",
    "        nodes=[\n",
    "            Add1_node,\n",
    "            Add2_node,\n",
    "            Abs_node,\n",
    "            Add3_node,\n",
    "            Round_node,\n",
    "        ],\n",
    "        name=\"simple_graph\",\n",
    "        inputs=[in1, in2, in3],\n",
    "        outputs=[out1],\n",
    "        value_info=[\n",
    "            onnx.helper.make_tensor_value_info(\"sum1\", onnx.TensorProto.FLOAT, [4, 4]),\n",
    "            onnx.helper.make_tensor_value_info(\"sum2\", onnx.TensorProto.FLOAT, [4, 4]),\n",
    "            onnx.helper.make_tensor_value_info(\"abs1\", onnx.TensorProto.FLOAT, [4, 4]),\n",
    "            onnx.helper.make_tensor_value_info(\"sum3\", onnx.TensorProto.FLOAT, [4, 4]),\n",
    "        ],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a model can be created from the graph and saved using the `.save` function. The model is saved in .onnx format and can be reloaded with `onnx.load()`. This also means that you can easily share your own model in .onnx format with others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = onnx.helper.make_model(graph, producer_name=\"simple-model\")\n",
    "onnx.save(onnx_model, 'simple_model.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Netron is used to visualize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'simple_model.onnx' at http://0.0.0.0:8081\n"
     ]
    }
   ],
   "source": [
    "import netron\n",
    "netron.start('simple_model.onnx', port=8081, host=\"0.0.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"http://0.0.0.0:8081/\" style=\"position: relative; width: 100%;\" height=\"400\"></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe src=\"http://0.0.0.0:8081/\" style=\"position: relative; width: 100%;\" height=\"400\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Netron also allows you to interactively explore the model. If you click on a node, the node attributes will be displayed. \n",
    "\n",
    "In order to test the resulting model, a function is first written in Python that calculates the expected output. Because numpy arrays are to be used, numpy is imported first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def expected_output(in1, in2, in3):\n",
    "    sum1 = np.add(in1, in2)\n",
    "    sum2 = np.add(sum1, in3)\n",
    "    abs1 = np.absolute(sum2)\n",
    "    sum3 = np.add(abs1, abs1)\n",
    "    return np.round(sum3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the values for the three inputs are calculated. Random numbers are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "in1_values =np.asarray(np.random.uniform(low=-5, high=5, size=(4,4)), dtype=np.float32)\n",
    "in2_values = np.asarray(np.random.uniform(low=-5, high=5, size=(4,4)), dtype=np.float32)\n",
    "in3_values = np.asarray(np.random.uniform(low=-5, high=5, size=(4,4)), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily pass the values to the function we just wrote to calculate the expected result. For the created model the inputs must be summarized in a dictionary, which is then passed on to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dict = {}\n",
    "input_dict[\"in1\"] = in1_values\n",
    "input_dict[\"in2\"] = in2_values\n",
    "input_dict[\"in3\"] = in3_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as rt\n",
    "\n",
    "sess = rt.InferenceSession(onnx_model.SerializeToString())\n",
    "output = sess.run(None, input_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output of the ONNX model is: \n",
      "[[12. 14. 13.  0.]\n",
      " [ 1.  1. 17.  5.]\n",
      " [ 5. 11.  3.  4.]\n",
      " [ 6. 14. 13.  4.]]\n",
      "\n",
      "The output of the reference function is: \n",
      "[[12. 14. 13.  0.]\n",
      " [ 1.  1. 17.  5.]\n",
      " [ 5. 11.  3.  4.]\n",
      " [ 6. 14. 13.  4.]]\n",
      "\n",
      "The results are the same!\n"
     ]
    }
   ],
   "source": [
    "ref_output= expected_output(in1_values, in2_values, in3_values)\n",
    "print(\"The output of the ONNX model is: \\n{}\".format(output[0]))\n",
    "print(\"\\nThe output of the reference function is: \\n{}\".format(ref_output))\n",
    "\n",
    "if (output[0] == ref_output).all():\n",
    "    print(\"\\nThe results are the same!\")\n",
    "else:\n",
    "    raise Exception(\"Something went wrong, the output of the model doesn't match the expected output!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have verified that the model works as we expected it to, we can continue working with the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
